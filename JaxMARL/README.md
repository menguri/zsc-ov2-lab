<h1 align="center">JaxMARL</h1>

<p align="center">
       <a href="https://pypi.python.org/pypi/jaxmarl">
        <img src="https://img.shields.io/pypi/pyversions/jaxmarl.svg" /></a>
       <a href="https://badge.fury.io/py/jaxmarl">
        <img src="https://badge.fury.io/py/jaxmarl.svg" /></a>
       <a href= "https://github.com/FLAIROx/JaxMARL/blob/main/LICENSE">
        <img src="https://img.shields.io/badge/license-Apache2.0-blue.svg" /></a>
       <a href= "https://colab.research.google.com/github/FLAIROx/JaxMARL/blob/main/jaxmarl/tutorials/JaxMARL_Walkthrough.ipynb">
        <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>
       <a href= "https://arxiv.org/abs/2311.10090">
        <img src="https://img.shields.io/badge/arXiv-2311.10090-b31b1b.svg" /></a>
       <a href= "https://jaxmarl.foersterlab.com/">
        <img src="https://img.shields.io/badge/docs-green" /></a>
       
</p>

[**Installation**](#install) | [**Quick Start**](#start) | [**Environments**](#environments) | [**Algorithms**](#algorithms) | [**Citation**](#cite)
---

<div class="collage">
    <div class="column" align="centre">
        <div class="row" align="centre">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/cramped_room.gif?raw=true" alt="Overcooked" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/mabrax.png?raw=true" alt="mabrax" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/storm.gif?raw=true" alt="STORM" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/hanabi.png?raw=true" alt="hanabi" width="20%">
        </div>
        <div class="row" align="centre">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/coin_game.png?raw=true" alt="coin_game" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/qmix_MPE_simple_tag_v3.gif?raw=true" alt="MPE" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/jaxnav-ma.gif?raw=true" alt="jaxnav" width="20%">
            <img src="https://github.com/FLAIROx/JaxMARL/blob/main/docs/imgs/smax.gif?raw=true" alt="SMAX" width="20%">
        </div>
    </div>
</div>

## Multi-Agent Reinforcement Learning in JAX

ğŸ‰ **Update: JaxMARL was accepted at NeurIPS 2024 on Datasets and Benchmarks Track. See you in Vacouver!**

JaxMARL combines ease-of-use with GPU-enabled efficiency, and supports a wide range of commonly used MARL environments as well as popular baseline algorithms. Our aim is for one library that enables thorough evaluation of MARL methods across a wide range of tasks and against relevant baselines. We also introduce SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. 

For more details, take a look at our [blog post](https://blog.foersterlab.com/jaxmarl/) or our [Colab notebook](https://colab.research.google.com/github/FLAIROx/JaxMARL/blob/main/jaxmarl/tutorials/JaxMARL_Walkthrough.ipynb), which walks through the basic usage.

<h2 name="environments" id="environments">Environments ğŸŒ </h2>

| Environment | Reference | README | Summary |
| --- | --- | --- | --- |
| ğŸ”´ MPE | [Paper](https://arxiv.org/abs/1706.02275) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/mpe) | Communication orientated tasks in a multi-agent particle world
| ğŸ² Overcooked | [Paper](https://arxiv.org/abs/1910.05789) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/overcooked) | Fully-cooperative human-AI coordination tasks based on the homonyms video game | 
| ğŸ¦¾ Multi-Agent Brax | [Paper](https://arxiv.org/abs/2003.06709) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/mabrax) | Continuous multi-agent robotic control based on Brax, analogous to Multi-Agent MuJoCo |
| ğŸ† Hanabi | [Paper](https://arxiv.org/abs/1902.00506) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/hanabi) | Fully-cooperative partially-observable multiplayer card game |
| ğŸ‘¾ SMAX | Novel | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/smax) | Simplified cooperative StarCraft micro-management environment |
| ğŸ§® STORM: Spatial-Temporal Representations of Matrix Games | [Paper](https://openreview.net/forum?id=54F8woU8vhq) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/storm) | Matrix games represented as grid world scenarios
| ğŸ§­ JaxNav | [Paper](https://www.arxiv.org/abs/2408.15099) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/jaxnav) | 2D geometric navigation for differential drive robots
| ğŸª™ Coin Game | [Paper](https://arxiv.org/abs/1802.09640) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/coin_game) | Two-player grid world environment which emulates social dilemmas
| ğŸ’¡ Switch Riddle | [Paper](https://proceedings.neurips.cc/paper_files/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/switch_riddle) | Simple cooperative communication game included for debugging

 
<h2 name="algorithms" id="algorithms">Baseline Algorithms ğŸ¦‰ </h2>

We follow CleanRL's philosophy of providing single file implementations which can be found within the `baselines` directory. We use Hydra to manage our config files, with specifics explained in each algorithm's README. Most files include `wandb` logging code, this is disabled by default but can be enabled within the file's config.

| Algorithm | Reference | README | 
| --- | --- | --- | 
| IPPO | [Paper](https://arxiv.org/pdf/2011.09533.pdf) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/IPPO) | 
| MAPPO | [Paper](https://arxiv.org/abs/2103.01955) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/MAPPO) | 
| IQL | [Paper](https://arxiv.org/abs/1312.5602v1) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/QLearning) | 
| VDN | [Paper](https://arxiv.org/abs/1706.05296)  | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/QLearning) |
| QMIX | [Paper](https://arxiv.org/abs/1803.11485) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/QLearning) |
| TransfQMIX | [Paper](https://www.southampton.ac.uk/~eg/AAMAS2023/pdfs/p1679.pdf) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/QLearning) |
| SHAQ | [Paper](https://arxiv.org/abs/2105.15013) | [Source](https://github.com/FLAIROx/JaxMARL/tree/main/baselines/QLearning) |
| PQN-VDN | [Paper](https://arxiv.org/abs/2407.04811) | [Source](https://github.com/mttga/purejaxql) |

<h2 name="install" id="install">Installation ğŸ§— </h2>

**Environments** - Before installing, ensure you have the correct [JAX installation](https://github.com/google/jax#installation) for your hardware accelerator. We have tested up to JAX version 0.4.25. The JaxMARL environments can be installed directly from PyPi:

```
pip install jaxmarl 
```

**Algorithms** - If you would like to also run the algorithms, install the source code as follows:

1. Clone the repository:
    ```
    git clone https://github.com/FLAIROx/JaxMARL.git && cd JaxMARL
    ```
2. Install requirements:
    ``` 
    pip install -e .[algs]
    export PYTHONPATH=./JaxMARL:$PYTHONPATH
    ```
3. For the fastest start, we reccoment using our Dockerfile, the usage of which is outlined below.

**Development** - If you would like to run our test suite, install the additonal dependencies with:
 `pip install -e .[dev]`, after cloning the repository.

<h2 name="start" id="start">Quick Start ğŸš€ </h2>

We take inspiration from the [PettingZoo](https://github.com/Farama-Foundation/PettingZoo) and [Gymnax](https://github.com/RobertTLange/gymnax) interfaces. You can try out training an agent in our [Colab notebook](https://colab.research.google.com/github/FLAIROx/JaxMARL/blob/main/jaxmarl/tutorials/JaxMARL_Walkthrough.ipynb). Further introduction scripts can be found [here](https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/tutorials).

### Basic JaxMARL API  Usage ğŸ–¥ï¸

Actions, observations, rewards and done values are passed as dictionaries keyed by agent name, allowing for differing action and observation spaces. The done dictionary contains an additional `"__all__"` key, specifying whether the episode has ended. We follow a parallel structure, with each agent passing an action at each timestep. For asynchronous games, such as Hanabi, a dummy action is passed for agents not acting at a given timestep.

```python 
import jax
from jaxmarl import make

key = jax.random.PRNGKey(0)
key, key_reset, key_act, key_step = jax.random.split(key, 4)

# Initialise environment.
env = make('MPE_simple_world_comm_v3')

# Reset the environment.
obs, state = env.reset(key_reset)

# Sample random actions.
key_act = jax.random.split(key_act, env.num_agents)
actions = {agent: env.action_space(agent).sample(key_act[i]) for i, agent in enumerate(env.agents)}

# Perform the step transition.
obs, state, reward, done, infos = env.step(key_step, state, actions)
```

### Dockerfile ğŸ‹
To help get experiments up and running we include a [Dockerfile](https://github.com/FLAIROx/JaxMARL/blob/main/Dockerfile) and its corresponding [Makefile](https://github.com/FLAIROx/JaxMARL/blob/main/Makefile). With Docker and the [Nvidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html) installed, the container can be built with:
```
make build
```
The built container can then be run:
```
make run
```

## Contributing ğŸ”¨
Please contribute! Please take a look at our [contributing guide](https://github.com/FLAIROx/JaxMARL/blob/main/CONTRIBUTING.md) for how to add an environment/algorithm or submit a bug report. Our roadmap also lives there.

## OvercookedV2 PANIC Robustness í™•ì¥ ğŸ²ğŸ”¥

> ì´ ì„¹ì…˜ì€ ë³¸ í¬í¬(`experiments/overcooked_v2_experiments/ppo`)ì—ì„œ ì¶”ê°€ëœ OvercookedV2ìš© ê°•ê±´ì„± í‰ê°€ ê¸°ëŠ¥(PANIC)ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ì›ë³¸ JaxMARL ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ ë°°í¬ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### ëª©ì 
ì‹¤ì œ í˜‘ë ¥ ìƒí™©ì—ì„œ íŒŒíŠ¸ë„ˆ(íŒ€ë©”ì´íŠ¸)ê°€ ìˆœê°„ì ìœ¼ë¡œ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ì˜ëª»ëœ í–‰ë™ì„ í•  ë•Œ Ego ì •ì±…ì´ ì–¼ë§ˆë‚˜ ê°•ê±´í•œì§€(ì„±ëŠ¥ ì €í•˜ ì–‘ìƒ, íšŒë³µë ¥, ì‹¤íŒ¨ íŒ¨í„´)ë¥¼ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•œ ì‹¤í—˜ì  ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´
ê° ë²¡í„°í™”ëœ í™˜ê²½ì˜ ì—í”¼ì†Œë“œë§ˆë‹¤ í•œ ì—ì´ì „íŠ¸ë¥¼ ë¬´ì‘ìœ„(2ì¸ í™˜ê²½ì€ Bernoulli(0.5) â†’ {0,1})ë¡œ ì„ íƒí•˜ì—¬, ì„¤ì •ëœ ì—í”¼ì†Œë“œ ë¡œì»¬ ìŠ¤í… êµ¬ê°„(`[panic.start_step, panic.start_step + panic.duration)`) ë™ì•ˆ ê·¸ ì—ì´ì „íŠ¸ì˜ ì•¡ì…˜ì„ ê· ì¼ ë‚œìˆ˜ë¡œ êµë€(override)í•©ë‹ˆë‹¤. êµë€ëœ ì•¡ì…˜ì— ëŒ€í•´ PPOì˜ `log_prob`ë¥¼ ì¬ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

### Hydra ì„¤ì • ì˜ˆì‹œ
```bash
python run.py \
    +panic.enabled=true \
    +panic.start_step=50 \
    +panic.duration=30 \
    env=overcooked_v2_rnn_sp
```

ì„¤ì • í‚¤:
- `panic.enabled` (bool): ê¸°ëŠ¥ on/off.
- `panic.start_step` (int): ì—í”¼ì†Œë“œ ë¡œì»¬ ìŠ¤í… ê¸°ì¤€ ì‹œì‘ ì§€ì .
- `panic.duration` (int): í™œì„± ì§€ì† ê¸¸ì´(ìŠ¤í… ìˆ˜). 0 ë˜ëŠ” ìŒìˆ˜ì´ë©´ ìë™ ë¹„í™œì„±(no-op).

### ë™ì‘ ìƒì„¸
1. ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ í™˜ê²½ë³„ë¡œ êµë€ ëŒ€ìƒ ì—ì´ì „íŠ¸ ì¸ë±ìŠ¤ ë°°ì—´(`panic_partner_indices`)ì„ ìƒ˜í”Œë§. ë¹„í™œì„± ì‹œ -1 ì €ì¥.
2. ë§¤ ìŠ¤í…ì—ì„œ `episode_step`ê°€ ì§€ì •ëœ êµ¬ê°„ì— ì†í•˜ê³  ëŒ€ìƒ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•˜ë©´ í•´ë‹¹ ì—ì´ì „íŠ¸ ì•¡ì…˜ì„ `Uniform(0, num_actions)` ë‚œìˆ˜ë¡œ ë®ì–´ì”€.
3. êµë€ í›„ ì •ì±… ë¶„í¬ë¡œë¶€í„° `log_prob` ì¬ê³„ì‚° â†’ PPO ratio ì¼ê´€ì„± ìœ ì§€.
4. ì¢…ë£Œí•œ ì—í”¼ì†Œë“œì— ëŒ€í•´ PANIC ê´€ë ¨ per-episode ëˆ„ì ì¹˜ë¥¼ ì´ê³„ë¡œ ë°˜ì˜ í›„ ë¦¬ì…‹.

### ìˆ˜ì§‘/ë¡œê¹… ë©”íŠ¸ë¦­ (wandb)
ê¸°ëŠ¥ì´ ë¹„í™œì„±(`enabled=false` ë˜ëŠ” `duration<=0`)ì´ë©´ ì•„ë˜ ë©”íŠ¸ë¦­ì€ ì „í˜€ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

| Key | ì •ì˜ |
| --- | --- |
| `panic/episodes_finished` | PANIC ì¶”ì  ë™ì•ˆ ì¢…ë£Œëœ ì—í”¼ì†Œë“œ ìˆ˜ í•©ê³„ |
| `panic/total_actions` | PANIC ì°½ì—ì„œ êµë€ëœ(override ë°œìƒ) ìŠ¤í… ì´í•© |
| `panic/total_reward` | PANIC í™œì„± êµ¬ê°„ì—ì„œ íŒ€(ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸) ë³´ìƒ ëˆ„ì  |
| `panic/total_deliveries` | PANIC í™œì„± êµ¬ê°„ì—ì„œ ì˜¬ë°”ë¥¸ ë°°ë‹¬(+DELIVERY_REWARD) íšŸìˆ˜ |
| `panic/total_wrong_deliveries` | PANIC í™œì„± êµ¬ê°„ì—ì„œ ì˜ëª»ëœ ë°°ë‹¬(-DELIVERY_REWARD) íšŸìˆ˜ |
| `panic/actions_per_episode` | `total_actions / max(episodes_finished,1)` |
| `panic/reward_per_episode` | `total_reward / max(episodes_finished,1)` |
| `panic/deliveries_per_episode` | `total_deliveries / max(episodes_finished,1)` |
| `panic/wrong_deliveries_per_episode` | `total_wrong_deliveries / max(episodes_finished,1)` |

### êµ¬í˜„ íŒŒì¼
- `ippo.py`: PANIC ì°½ í™œì„± íŒì •, ì•¡ì…˜ override ì ìš©, per-step/episode ìƒíƒœ ìœ ì§€.
- `panic_utils.py`: ëŒ€ìƒ ì„ íƒ, ì•¡ì…˜ êµë€, ëˆ„ì /ì§‘ê³„ í•¨ìˆ˜ (í•œêµ­ì–´ ë¼ì¸ë³„ ì£¼ì„ í¬í•¨).

### ì„±ëŠ¥/ì˜¤ë²„í—¤ë“œ
ë¹„í™œì„± ì‹œ ì¡°ê±´ë¬¸ ì¡°ê¸° íƒˆì¶œë¡œ ì¶”ê°€ ì—°ì‚°/ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œëŠ” ìµœì†Œí™”ë©ë‹ˆë‹¤(ì£¼ë¡œ ëª‡ ê°œì˜ zero ë°°ì—´ ìœ ì§€). JIT ì»´íŒŒì¼ í›„ êµë€ ë¶„ê¸° ë¹„ìš©ì€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.

### ì—ì§€ ì¼€ì´ìŠ¤ & í–¥í›„ í™•ì¥
- `start_step`ê°€ ì‹¤ì œ ì—í”¼ì†Œë“œ ê¸¸ì´ë³´ë‹¤ í¬ë©´ êµë€ì€ ë°œìƒí•˜ì§€ ì•ŠìŒ(ìë™ no-op).
- `duration=0` â†’ ì „ì²´ ë©”íŠ¸ë¦­ ë¹„ìƒì„±.
- N>2 ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ í™•ì¥: í˜„ì¬ 2ì¸ í™˜ê²½ì€ Bernoulli(0.5), ì¼ë°˜í™” ì‹œ Uniform over agents ì‚¬ìš© ê°€ëŠ¥ (ì½”ë“œì— fallback í¬í•¨).
- í•©ë²•/legal ì•¡ì…˜ ì§‘í•© í•„í„°ë§: í–¥í›„ í™˜ê²½ë³„ ì œí•œ ì•¡ì…˜ ì¡´ì¬ ì‹œ ìƒ˜í”Œë§ì„ ì „ì²´ ê³µê°„ ëŒ€ì‹  í•©ë²• ì§‘í•©ìœ¼ë¡œ ì¢í ìˆ˜ ìˆìŒ.

### ì—°êµ¬ í™œìš© ì‹œ ê¶Œì¥ ë¶„ì„
1. `panic/actions_per_episode` vs ê¸°ë³¸ ì„±ëŠ¥ ì €í•˜ìœ¨.
2. `panic/wrong_deliveries_per_episode` ê¸‰ì¦ êµ¬ê°„ íƒì§€ â†’ íŒŒíŠ¸ë„ˆ ë…¸ì´ì¦ˆ ë¯¼ê°ë„.
3. `panic/reward_per_episode` íšŒë³µ ê³¡ì„  ì¶”ì  â†’ ì •ì±… íšŒë³µë ¥(resilience) ì •ëŸ‰í™”.

### ê°„ë‹¨í•œ ì˜ì‚¬ì½”ë“œ
```python
if panic_enabled and 0 < duration and start_step <= ep_step < start_step + duration:
        action[target_agent, env_idx] = random.randint(0, num_actions)
        log_prob = pi.log_prob(flatten(action))
```

### ì¸ìš©/ëª…ì‹œ
ë…¼ë¬¸ ë˜ëŠ” ë³´ê³ ì„œì—ì„œ ì‚¬ìš© ì‹œ â€œOvercookedV2 PANIC Robustness Extension (uniform partner action perturbation)â€ í˜•íƒœë¡œ ëª…ì‹œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.


<h2 name="cite" id="cite">Citing JaxMARL ğŸ“œ </h2>
If you use JaxMARL in your work, please cite us as follows:

```
@article{flair2023jaxmarl,
      title={JaxMARL: Multi-Agent RL Environments in JAX},
      author={Alexander Rutherford and Benjamin Ellis and Matteo Gallici and Jonathan Cook and Andrei Lupu and Gardar Ingvarsson and Timon Willi and Akbir Khan and Christian Schroeder de Witt and Alexandra Souly and Saptarashmi Bandyopadhyay and Mikayel Samvelyan and Minqi Jiang and Robert Tjarko Lange and Shimon Whiteson and Bruno Lacerda and Nick Hawes and Tim Rocktaschel and Chris Lu and Jakob Nicolaus Foerster},
      journal={arXiv preprint arXiv:2311.10090},
      year={2023}
    }
```

## See Also ğŸ™Œ
There are a number of other libraries which inspired this work, we encourage you to take a look!

JAX-native algorithms:
- [Mava](https://github.com/instadeepai/Mava): JAX implementations of IPPO and MAPPO, two popular MARL algorithms.
- [PureJaxRL](https://github.com/luchris429/purejaxrl): JAX implementation of PPO, and demonstration of end-to-end JAX-based RL training.
- [Minimax](https://github.com/facebookresearch/minimax/): JAX implementations of autocurricula baselines for RL.
- [JaxIRL](https://github.com/FLAIROx/jaxirl?tab=readme-ov-file): JAX implementation of algorithms for inverse reinforcement learning.

JAX-native environments:
- [Gymnax](https://github.com/RobertTLange/gymnax): Implementations of classic RL tasks including classic control, bsuite and MinAtar.
- [Jumanji](https://github.com/instadeepai/jumanji): A diverse set of environments ranging from simple games to NP-hard combinatorial problems.
- [Pgx](https://github.com/sotetsuk/pgx): JAX implementations of classic board games, such as Chess, Go and Shogi.
- [Brax](https://github.com/google/brax): A fully differentiable physics engine written in JAX, features continuous control tasks.
- [XLand-MiniGrid](https://github.com/corl-team/xland-minigrid): Meta-RL gridworld environments inspired by XLand and MiniGrid.
- [Craftax](https://github.com/MichaelTMatthews/Craftax): (Crafter + NetHack) in JAX.